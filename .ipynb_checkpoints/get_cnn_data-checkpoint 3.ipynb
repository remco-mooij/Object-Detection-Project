{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from imutils import paths\n",
    "import cv2\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import base64\n",
    "from PIL import Image\n",
    "import selenium\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download more than 500 images of any given parameter\n",
    "def download_extended_urls(param):\n",
    "    google_url = \"https://www.google.com/imghp\"\n",
    "    driver = webdriver.Chrome()\n",
    "    try:\n",
    "        driver.get(google_url)\n",
    "        #browser.visit(google_url)\n",
    "        que=driver.find_element_by_xpath(\"//input[@name='q']\")\n",
    "        que.send_keys(param)\n",
    "        que.send_keys(Keys.RETURN)       \n",
    "        element = driver.find_element_by_tag_name(\"body\")\n",
    "        # Scroll down\n",
    "        for i in range(60):\n",
    "            element.send_keys(Keys.PAGE_DOWN)\n",
    "            time.sleep(0.3)\n",
    "        try:\n",
    "            driver.find_element_by_xpath('//*[@id=\"islmp\"]/div/div/div/div/div[5]/input').click()\n",
    "            for i in range(100):\n",
    "                element.send_keys(Keys.PAGE_DOWN)\n",
    "                time.sleep(0.3)  # bot id protection\n",
    "        except:\n",
    "            for i in range(30):\n",
    "                element.send_keys(Keys.PAGE_DOWN)\n",
    "                time.sleep(0.3)  # bot id protection\n",
    "\n",
    "        print(\"Reached end of Page.\")\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        source = driver.page_source #page source\n",
    "        #close the browser\n",
    "        driver.close()\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "        \n",
    "    url_list = []\n",
    "    soup = bs(source, \"html.parser\")\n",
    "    img = soup.find_all(\"img\", class_='rg_i Q4LuWd tx8vtf')\n",
    "    for i in img:\n",
    "        try:\n",
    "            url_list.append(i['src'])\n",
    "        except:\n",
    "            pass\n",
    "    with open(f'urls_for_training/{param}.txt', 'w') as filehandle:\n",
    "        for listitem in url_list:\n",
    "            filehandle.write('%s\\n' % listitem)\n",
    "    return url_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_img_from_url(param):\n",
    "    # grab the list of URLs then initialize the\n",
    "    # total number of images downloaded thus far\n",
    "    total = 0\n",
    "    # loop the URLs\n",
    "    for url in download_extended_urls(param):#[:70]:\n",
    "        try:\n",
    "            r = requests.get(url, timeout=60)\n",
    "            result_file = 'result_file'\n",
    "\n",
    "            with open(result_file, 'wb') as file_handler:\n",
    "                file_handler.write(r.content)\n",
    "            try:\n",
    "                os.mkdir(f\"image/{param}/\")\n",
    "            except:\n",
    "                pass\n",
    "            p = os.path.join(\"image\", param, \"{}.jpg\".format(str(total).zfill(8)))\n",
    "            Image.open(result_file).save(p)\n",
    "            os.remove(result_file)\n",
    "\n",
    "            # update the counter\n",
    "            print(\"[INFO] downloaded: {}\".format(p))\n",
    "        except:\n",
    "            try:       \n",
    "                z = re.findall(f'.+base64,(.+)',url)[0]\n",
    "                p = os.path.join(\"image\", param, \"{}.jpg\".format(str(total).zfill(8)))\n",
    "                im = Image.open(io.BytesIO(base64.b64decode(z))).save(p)\n",
    "            # handle if any exceptions are thrown during the download process\n",
    "            except:\n",
    "                print(\"[INFO] error downloading {}...skipping\".format(p))\n",
    "        total += 1\n",
    "    # loop over the image paths we just downloaded\n",
    "    for imagePath in paths.list_images(os.path.join(\"image\", param)):\n",
    "    # initialize if the image should be deleted or not\n",
    "        delete = False\n",
    "\n",
    "        # try to load the image\n",
    "        try:\n",
    "            image = cv2.imread(imagePath)\n",
    "            # if the image is `None` then we could not properly load it\n",
    "            # from disk, so delete it\n",
    "            if image is None:\n",
    "                print(\"None\")\n",
    "                delete = True\n",
    "        # if OpenCV cannot load the image then the image is likely\n",
    "        # corrupt so we should delete it\n",
    "        except:\n",
    "            print(\"Except\")\n",
    "            delete = True\n",
    "\n",
    "        # check to see if the image should be deleted\n",
    "        if delete:\n",
    "            print(\"[INFO] deleting {}\".format(imagePath))\n",
    "            os.remove(imagePath)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kitchen = \"Silverware organizer, Plates, Coffee mugs, Tongs, Spatulas, Ladles and Skimmer, Whisks,\\\n",
    "    Utensil holder, Cutting board, Pots and Pans, Baking sheets, Tupperware,\\\n",
    "    Bottle Opener & Corkscrew, Measuring Cups, spoons, Toaster oven, Microwave, Coffee Maker,\\\n",
    "    Coffee Grinder, Kettle, Blender, Colander, Salad spinner, Trash can, Trash bags, Step stool,\\\n",
    "    Magnetic hooks, Sponges, Dish soap, Drying rack, Dish towels, Napkins, Paper Towels,\\\n",
    "    Aluminum foil, wax paper, plastic wrap, Parchment paper, Plastic bags, Ice cube trays,\\\n",
    "    Can opener, Oven mits, Cooling mat, Rags, Paper towels, Windex, Clorox wipes, Stove cleaner, Rubber Gloves\"\n",
    "\n",
    "\n",
    "\n",
    "coco = \"bottle, wine glass, cup, fork, knife, spoon, bowl, diningtable, microwave, oven, toaster, \\\n",
    "       sink, refrigerator, scissors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = \"dog\"\n",
    "google_url = \"https://www.google.com/imghp\"\n",
    "ChromeOptions = webdriver.ChromeOptions()\n",
    "ChromeOptions.set_headless()\n",
    "driver = webdriver.Chrome(chrome_options=ChromeOptions)\n",
    "actionChains = ActionChains(driver)\n",
    "try:\n",
    "    driver.get(google_url)\n",
    "    #browser.visit(google_url)\n",
    "    que=driver.find_element_by_xpath(\"//input[@name='q']\")\n",
    "    que.send_keys(param)\n",
    "    que.send_keys(Keys.RETURN)       \n",
    "    element = driver.find_element_by_tag_name(\"body\")\n",
    "    # Scroll down\n",
    "#     for i in range(5):\n",
    "#         element.send_keys(Keys.PAGE_DOWN)\n",
    "#         time.sleep(0.3)\n",
    "#     try:\n",
    "#         driver.find_element_by_xpath('//*[@id=\"islmp\"]/div/div/div/div/div[5]/input').click()\n",
    "#         for i in range(5):\n",
    "#             element.send_keys(Keys.PAGE_DOWN)\n",
    "#             time.sleep(0.3)  # bot id protection\n",
    "#     except:\n",
    "#         for i in range(5):\n",
    "#             element.send_keys(Keys.PAGE_DOWN)\n",
    "#             time.sleep(0.3)  # bot id protection\n",
    "    driver.find_element_by_xpath('/html/body/div[2]/c-wiz/div[3]/div[1]/div/div/div/div/div[1]/div[1]/div[1]/a[1]/div[1]/img').click()\n",
    "    #img = driver.find_element_by_xpath('/html/body/div[2]/c-wiz/div[3]/div[2]/div[3]/div/div/div[3]/div[2]/div/div[1]/div[1]/div/div[2]/a/img')\n",
    "    \n",
    "    img = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, '/html/body/div[2]/c-wiz/div[3]/div[2]/div[3]/div/div/div[3]/div[2]/div/div[1]/div[1]/div/div[2]/a/img')))\n",
    "    src = img.get_attribute('src')\n",
    "    print(src)\n",
    "    print(\"Reached end of Page.\")\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    source = driver.page_source #page source\n",
    "    soup = bs(source, \"html.parser\")\n",
    "    #close the browser\n",
    "    #driver.close()\n",
    "except Error as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = driver.execute_script(\"return document.getElementsByTagName('img')\")\n",
    "for image in range(0, len(all_images)):\n",
    "    print(driver.execute_script(\"return document.getElementsByTagName('img')[{}].src\".format(image)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 168)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = re.findall(f'.+base64,(.+)',src)[0]\n",
    "#p = os.path.join(\"image\", param, \"{}.jpg\".format(str(total).zfill(8)))\n",
    "im = Image.open(io.BytesIO(base64.b64decode(z)))\n",
    "im.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 168)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.open(\"doggy.jpg\").size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(257, 257)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.open(\"55043370.jpg\").size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = []\n",
    "soup = bs(source, \"html.parser\")\n",
    "img = soup.find_all(\"img\", class_='rg_i Q4LuWd tx8vtf')\n",
    "for i in img:\n",
    "    try:\n",
    "        url_list.append(i['src'])\n",
    "    except:\n",
    "        pass\n",
    "with open(f'urls_for_training/{param}.txt', 'w') as filehandle:\n",
    "    for listitem in url_list:\n",
    "        filehandle.write('%s\\n' % listitem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_pages = 5\n",
    "query = 'pots'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_amazon_data(query,no_of_pages):\n",
    "    amazon_dict = {\n",
    "                    \"featured\": {\n",
    "                        \"products_name\":[],\n",
    "                        \"prices\": [],\n",
    "                        \"ratings\":[],\n",
    "                        \"products_img\":[],\n",
    "                        \"products_url\":[],\n",
    "                        \"prime_delivery\":[]\n",
    "                    }, \n",
    "                    \"low_to_high\": {\n",
    "                        \"products_name\":[],\n",
    "                        \"prices\": [],\n",
    "                        \"ratings\":[],\n",
    "                        \"products_img\":[],\n",
    "                        \"products_url\":[],\n",
    "                        \"prime_delivery\":[]\n",
    "                    },\n",
    "                    \"high_to_low\": {\n",
    "                        \"products_name\":[],\n",
    "                        \"prices\": [],\n",
    "                        \"ratings\":[],\n",
    "                        \"products_img\":[],\n",
    "                        \"products_url\":[],\n",
    "                        \"prime_delivery\":[]\n",
    "                    }, \n",
    "                    \"average_customer_review\": {\n",
    "                        \"products_name\":[],\n",
    "                        \"prices\": [],\n",
    "                        \"ratings\":[],\n",
    "                        \"products_img\":[],\n",
    "                        \"products_url\":[],\n",
    "                        \"prime_delivery\":[]\n",
    "                    },\n",
    "                    \"newest_arrival\": {\n",
    "                        \"products_name\":[],\n",
    "                        \"prices\": [],\n",
    "                        \"ratings\":[],\n",
    "                        \"products_img\":[],\n",
    "                        \"products_url\":[],\n",
    "                        \"prime_delivery\":[]\n",
    "                    }\n",
    "                  }\n",
    "\n",
    "    executable_path = {\"executable_path\": \"/usr/local/bin/chromedriver\"}\n",
    "    browser = Browser(\"chrome\", **executable_path, headless=True)\n",
    "    for j in list(range(5)):\n",
    "        for i in list(range(no_of_pages)):\n",
    "            news_url = f\"https://www.amazon.com/s?k={query}&page={i}\"\n",
    "            try:\n",
    "                browser.visit(news_url)\n",
    "                browser.find_by_xpath('//*[@id=\"a-autoid-0\"]').click()\n",
    "                browser.find_by_xpath(f'//*[@id=\"s-result-sort-select_{str(j)}\"]').click()\n",
    "                html = browser.html\n",
    "                soup = bs(html, \"html.parser\")\n",
    "\n",
    "                for d in soup.findAll('div', attrs={'class':'sg-col-4-of-24 sg-col-4-of-12 sg-col-4-of-36 s-result-item s-asin sg-col-4-of-28 sg-col-4-of-16 sg-col sg-col-4-of-20 sg-col-4-of-32'}):\n",
    "                    name = d.find('span', attrs={'class':'a-size-base-plus a-color-base a-text-normal'})\n",
    "                    price = d.find('span', attrs={'class':'a-offscreen'})\n",
    "                    rating = d.find('span', attrs={'class':'a-icon-alt'})\n",
    "                    prime = d.find('i', attrs={'class':'a-icon a-icon-prime a-icon-medium'})\n",
    "                    img = d.find('img', attrs={'class':'s-image'})\n",
    "                    url = d.find('a', attrs={'class':'a-link-normal a-text-normal'})\n",
    "\n",
    "                    if name is not None and price is not None:\n",
    "                        amazon_dict[list(amazon_dict.keys())[j]][\"products_name\"].append(name.text)\n",
    "                        amazon_dict[list(amazon_dict.keys())[j]][\"prices\"].append(price.text)\n",
    "\n",
    "                        if rating is not None:\n",
    "                            amazon_dict[list(amazon_dict.keys())[j]][\"ratings\"].append(rating.text)\n",
    "                        else:\n",
    "                            amazon_dict[list(amazon_dict.keys())[j]][\"ratings\"].append(\"Missing\")\n",
    "\n",
    "                        if img is not None:\n",
    "                            amazon_dict[list(amazon_dict.keys())[j]][\"products_img\"].append(img[\"src\"])\n",
    "                        else:\n",
    "                            amazon_dict[list(amazon_dict.keys())[j]][\"products_img\"].append(\" \")\n",
    "\n",
    "                        if url is not None:\n",
    "                            amazon_dict[list(amazon_dict.keys())[j]][\"products_url\"].append(\"https://amazon.com\"+url[\"href\"])\n",
    "                        else:\n",
    "                            amazon_dict[list(amazon_dict.keys())[j]][\"products_url\"].append(\" \")\n",
    "\n",
    "                        if prime is not None:\n",
    "                            amazon_dict[list(amazon_dict.keys())[j]][\"prime_delivery\"].append(\"Prime Delivery\")\n",
    "                        else:\n",
    "                            amazon_dict[list(amazon_dict.keys())[j]][\"prime_delivery\"].append(\"Not Prime Delivery\")\n",
    "                        \n",
    "            except Error as e:\n",
    "                print(e)\n",
    "    browser.quit()\n",
    "    return amazon_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Walmart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_walmart_data(query,no_of_pages):\n",
    "    drop_down = [\n",
    "        \"best_match--Best Match\",\n",
    "        \"best_seller--Best Sellers\",\n",
    "        \"price_low--Price: low to high\",\n",
    "        \"price_high--Price: high to low\",\n",
    "        \"rating_high--Highest Rating\",\n",
    "        \"new--New\"\n",
    "    ]\n",
    "    walmart_dict = {\n",
    "                    \"best_match\": {\n",
    "                        \"products_name\":[],\n",
    "                        \"prices\": [],\n",
    "                        \"ratings\":[],\n",
    "                        \"no_of_reviews\":[],\n",
    "                        \"products_img\":[],\n",
    "                        \"products_url\":[],\n",
    "                        \"delivery_info\":[]\n",
    "                    },\n",
    "                    \"best_sellers\": {\n",
    "                        \"products_name\":[],\n",
    "                        \"prices\": [],\n",
    "                        \"ratings\":[],\n",
    "                        \"no_of_reviews\":[],\n",
    "                        \"products_img\":[],\n",
    "                        \"products_url\":[],\n",
    "                        \"delivery_info\":[]\n",
    "                    },\n",
    "                    \"high_to_low\": {\n",
    "                        \"products_name\":[],\n",
    "                        \"prices\": [],\n",
    "                        \"ratings\":[],\n",
    "                        \"no_of_reviews\":[],\n",
    "                        \"products_img\":[],\n",
    "                        \"products_url\":[],\n",
    "                        \"delivery_info\":[]\n",
    "                    }, \n",
    "                    \"low_to_high\": {\n",
    "                        \"products_name\":[],\n",
    "                        \"prices\": [],\n",
    "                        \"ratings\":[],\n",
    "                        \"no_of_reviews\":[],\n",
    "                        \"products_img\":[],\n",
    "                        \"products_url\":[],\n",
    "                        \"delivery_info\":[]\n",
    "                    },\n",
    "                    \"highest_rating\": {\n",
    "                        \"products_name\":[],\n",
    "                        \"prices\": [],\n",
    "                        \"ratings\":[],\n",
    "                        \"no_of_reviews\":[],\n",
    "                        \"products_img\":[],\n",
    "                        \"products_url\":[],\n",
    "                        \"delivery_info\":[]\n",
    "                    },\n",
    "                    \"newest_arrival\": {\n",
    "                        \"products_name\":[],\n",
    "                        \"prices\": [],\n",
    "                        \"ratings\":[],\n",
    "                        \"no_of_reviews\":[],\n",
    "                        \"products_img\":[],\n",
    "                        \"products_url\":[],\n",
    "                        \"delivery_info\":[]\n",
    "                    }\n",
    "                  }\n",
    "    \n",
    "    executable_path = {\"executable_path\": \"/usr/local/bin/chromedriver\"}\n",
    "    browser = Browser(\"chrome\", **executable_path, headless=True)\n",
    "    for i in list(range(1,no_of_pages+1)):\n",
    "        url = f\"https://www.walmart.com/search/?page={i}&query={query}\"\n",
    "        browser.visit(url)\n",
    "        for j in list(range(6)): \n",
    "            browser.find_by_xpath('//*[@id=\"SearchContainer\"]/div/div[2]/div/div[2]/div/div[2]/div[1]/div/select').click()\n",
    "            browser.find_by_value(drop_down[j]).click()\n",
    "            html = browser.html\n",
    "            soup = bs(html, \"html.parser\")\n",
    "\n",
    "            for d in soup.findAll('li'):\n",
    "                try:\n",
    "                    name = d.find('div', attrs={'class':'search-result-product-title gridview'})\n",
    "                    price = d.find('span', attrs={'class':'price display-inline-block arrange-fit price price-main'})\\\n",
    "                        .find('span', attrs={'class':'visuallyhidden'})\n",
    "                    rating = d.find('span', attrs={'class':'seo-avg-rating'})\n",
    "                    review = d.find('span', attrs={'class':'seo-review-count'})\n",
    "                    delivery = d.find('div', attrs={'class':'search-result-product-shipping-details gridview'})\n",
    "                    img = d.find('div', attrs={'class':'orientation-square'}).find(\"img\")\n",
    "                    url = d.find('a', attrs={'class':'product-title-link line-clamp line-clamp-2 truncate-title'})\n",
    "\n",
    "                    if name.find(\"a\").find(\"span\") is not None and price is not None:\n",
    "                        walmart_dict[list(walmart_dict.keys())[j]][\"products_name\"].append(name.find(\"a\").find(\"span\").text)\n",
    "                        walmart_dict[list(walmart_dict.keys())[j]][\"prices\"].append(price.text)\n",
    "\n",
    "                        if rating is not None:\n",
    "                            walmart_dict[list(walmart_dict.keys())[j]][\"ratings\"].append(rating.text)\n",
    "                        else:\n",
    "                            walmart_dict[list(walmart_dict.keys())[j]][\"ratings\"].append(\"Missing Ratings\")\n",
    "\n",
    "                        if review is not None:\n",
    "                            walmart_dict[list(walmart_dict.keys())[j]][\"no_of_reviews\"].append(rating.text)\n",
    "                        else:\n",
    "                            walmart_dict[list(walmart_dict.keys())[j]][\"no_of_reviews\"].append(\"Missing Reviews\")\n",
    "\n",
    "                        if img is not None:\n",
    "                            walmart_dict[list(walmart_dict.keys())[j]][\"products_img\"].append(img[\"data-image-src\"])\n",
    "                        else:\n",
    "                            walmart_dict[list(walmart_dict.keys())[j]][\"products_img\"].append(\" \")\n",
    "\n",
    "                        if url is not None:\n",
    "                            walmart_dict[list(walmart_dict.keys())[j]][\"products_url\"].append(\"https://walmart.com\"+url[\"href\"])\n",
    "                        else:\n",
    "                            walmart_dict[list(walmart_dict.keys())[j]][\"products_url\"].append(\" \")\n",
    "\n",
    "                        if delivery is not None:\n",
    "                            walmart_dict[list(walmart_dict.keys())[j]][\"delivery_info\"].append(delivery.text.replace(\"\\xa0\", \" \"))\n",
    "                        else:\n",
    "                            walmart_dict[list(walmart_dict.keys())[j]][\"delivery_info\"].append(\"Missing\")\n",
    "\n",
    "                except:\n",
    "                    pass\n",
    "    browser.quit()\n",
    "    return walmart_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = get_amazon_data('pencil',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = None\n",
    "str_error = None\n",
    "num_retries = 4\n",
    "for x in range(0, num_retries):  \n",
    "    try:\n",
    "        b = get_walmart_data(\"pencil\",1)\n",
    "    except Exception as str_error:\n",
    "        pass\n",
    "\n",
    "    if b == str_error:\n",
    "        sleep(0.2)  # wait before trying to fetch the data again\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>products_name</th>\n",
       "      <th>prices</th>\n",
       "      <th>ratings</th>\n",
       "      <th>products_img</th>\n",
       "      <th>products_url</th>\n",
       "      <th>prime_delivery</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [products_name, prices, ratings, products_img, products_url, prime_delivery]\n",
       "Index: []"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(a[\"high_to_low\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-518-ffc171080007>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'high_to_low'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(b['high_to_low'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(a[\"low_to_high\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(b['low_to_high'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python37364bitbasecondaf7339001cfe34b54ae1ffacfd6447597"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
